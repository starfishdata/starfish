{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-14 09:25:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.common.env_loader\u001b[0m | \u001b[34menv_loader.py:52\u001b[0m | \u001b[1mLoaded 9 environment variables from /Users/zhengisamazing/1.python_dir/starfish/.env\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starfish import StructuredLLM, data_factory\n",
    "from starfish.common.env_loader import load_env_file\n",
    "\n",
    "load_env_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured LLM - Single\n",
    "#### 1. Model provider LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, New York City has an estimated population of over 8.4 million people, making it the most populous city in the United States.'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\")\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù CONSTRUCTED MESSAGES:\n",
      "================================================================================\n",
      "\n",
      "Role: user\n",
      "Content:\n",
      "Facts about city New York.\n",
      "\n",
      "\n",
      "You are asked to generate exactly 5 records and please return the data in the following JSON format: \n",
      "[\n",
      "    {\n",
      "    \"question\": \"\"  //  (required),\n",
      "    \"answer\": \"\"  //  (required)\n",
      "    }\n",
      "    ...\n",
      "]\n",
      "\n",
      "Required fields: question, answer\n",
      "\n",
      "\n",
      "================================================================================\n",
      "End of prompt\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(first_llm.render_prompt_printable(city_name=\"New York\", num_records=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Customized Openai Compatible Model provider LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the nickname of New York City?',\n",
       "  'answer': 'The Big Apple'},\n",
       " {'question': 'Which famous statue is located in New York Harbor?',\n",
       "  'answer': 'The Statue of Liberty'},\n",
       " {'question': 'What is the name of the largest park in Manhattan?',\n",
       "  'answer': 'Central Park'},\n",
       " {'question': 'Which borough of New York City is known for its diverse culture and arts scene?',\n",
       "  'answer': 'Brooklyn'},\n",
       " {'question': 'What is the name of the famous theater district in New York City?',\n",
       "  'answer': 'Broadway'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_llm = StructuredLLM(\n",
    "    model_name=\"hyperbolic/deepseek-ai/DeepSeek-V3-0324\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{'name': 'question', 'type': 'str'}, \n",
    "                   {'name': 'answer', 'type': 'str'}],\n",
    "    model_kwargs={\"temperature\": 0.7}\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 23:58:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.proxy.litellm_adapter\u001b[0m | \u001b[34mlitellm_adapter.py:94\u001b[0m | \u001b[1mEnsuring Ollama model gemma3:1b is ready...\u001b[0m\n",
      "\u001b[32m2025-04-13 23:58:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:63\u001b[0m | \u001b[1mStarting Ollama server...\u001b[0m\n",
      "\u001b[32m2025-04-13 23:58:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:79\u001b[0m | \u001b[1mOllama server started successfully\u001b[0m\n",
      "\u001b[32m2025-04-13 23:58:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:129\u001b[0m | \u001b[1mFound model gemma3:1b\u001b[0m\n",
      "\u001b[32m2025-04-13 23:58:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:232\u001b[0m | \u001b[1mModel gemma3:1b is already available\u001b[0m\n",
      "\u001b[32m2025-04-13 23:58:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.proxy.litellm_adapter\u001b[0m | \u001b[34mlitellm_adapter.py:103\u001b[0m | \u001b[1mModel gemma3:1b is ready, making API call...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, the population of New York City is approximately 8.8 million people.'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Local model\n",
    "first_llm = StructuredLLM(\n",
    "    model_name=\"ollama/gemma3:1b\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-13 23:58:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:254\u001b[0m | \u001b[1mStopping Ollama server...\u001b[0m\n",
      "\u001b[32m2025-04-13 23:58:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:305\u001b[0m | \u001b[1mOllama server stopped successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Clean it up\n",
    "from starfish.core.llm.backend.ollama_adapter import stop_ollama_server\n",
    "await stop_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured LLM - Workflow\n",
    "#### 1. Two LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, New York City has an estimated population of over 8.4 million people.',\n",
       "  'accuracy': 9,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 2},\n",
       " {'question': 'What is the official nickname of New York City?',\n",
       "  'answer': \"New York City is commonly known as 'The Big Apple.'\",\n",
       "  'accuracy': 10,\n",
       "  'funny': 3,\n",
       "  'conciseness': 10,\n",
       "  'rank': 1},\n",
       " {'question': 'Which river borders New York City to the west?',\n",
       "  'answer': 'The Hudson River borders New York City to the west.',\n",
       "  'accuracy': 9,\n",
       "  'funny': 1,\n",
       "  'conciseness': 9,\n",
       "  'rank': 4},\n",
       " {'question': 'What is the tallest building in New York City?',\n",
       "  'answer': 'As of 2023, One World Trade Center is the tallest building in New York City, standing at 1,776 feet.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 10,\n",
       "  'rank': 1},\n",
       " {'question': 'What famous park is located in the heart of Manhattan?',\n",
       "  'answer': 'Central Park is the famous park located in the heart of Manhattan, spanning over 843 acres.',\n",
       "  'accuracy': 9,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 3}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starfish.core.llm.utils import merge_structured_outputs\n",
    "from starfish import StructuredLLM \n",
    "first_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "\n",
    "\n",
    "second_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"\"\"You will be given a list of question and answer pairs, \n",
    "please rate each individually about its accuracy, funny and conciseness. \n",
    "rating are from 1 to 10, 1 being the worst and 10 being the best. \n",
    "lets also rank them among themself so from 1 being the best.\n",
    "Here is question and answer pairs: {{QnA_pairs}}\"\"\",\n",
    "    output_schema=[{\"name\": \"accuracy\", \"type\": \"int\"}, {\"name\": \"funny\", \"type\": \"int\"}, {\"name\": \"conciseness\", \"type\": \"int\"}, {\"name\": \"rank\", \"type\": \"int\"}],\n",
    "    model_kwargs={\"temperature\": 1},\n",
    ")\n",
    "\n",
    "second_response = await second_llm.run(QnA_pairs=first_response.data)\n",
    "\n",
    "### Merge result:\n",
    "merge_structured_outputs(first_response.data, second_response.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù CONSTRUCTED MESSAGES:\n",
      "================================================================================\n",
      "\n",
      "Role: user\n",
      "Content:\n",
      "Facts about city New York.\n",
      "\n",
      "\n",
      "You are asked to generate exactly 5 records and please return the data in the following JSON format: \n",
      "[\n",
      "    {\n",
      "    \"question\": \"\"  //  (required),\n",
      "    \"answer\": \"\"  //  (required)\n",
      "    }\n",
      "    ...\n",
      "]\n",
      "\n",
      "Required fields: question, answer\n",
      "\n",
      "\n",
      "================================================================================\n",
      "End of prompt\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(first_llm.render_prompt_printable(city_name=\"New York\", num_records=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù CONSTRUCTED MESSAGES:\n",
      "================================================================================\n",
      "\n",
      "Role: user\n",
      "Content:\n",
      "You will be given a list of question and answer pairs, \n",
      "please rate each individually about its accuracy, funny and conciseness. \n",
      "rating are from 1 to 10, 1 being the worst and 10 being the best. \n",
      "lets also rank them among themself so from 1 being the best.\n",
      "Here is question and answer pairs: |QnA_pairs| : [{\"question\": \"What is the population of New York City?\", \"answer\": \"As of 2023, New York City has an estimated population of over 8.4 million people.\"}, {\"question\": \"What is the official nickname of New York City?\", \"answer\": \"New York City is commonly known as 'The Big Apple.'\"}, {\"question\": \"Which river borders New York City to the west?\", \"answer\": \"The Hudson River borders New York City to the west.\"}, {\"question\": \"What is the tallest building in New York City?\", \"answer\": \"As of 2023, One World Trade Center is the tallest building in New York City, standing at 1,776 feet.\"}, {\"question\": \"What famous park is located in the heart of Manhattan?\", \"answer\": \"Central Park is the famous park located in the heart of Manhattan, spanning over 843 acres.\"}]\n",
      "\n",
      "\n",
      "Additional Instructions:\n",
      "\n",
      "You are provided with a list named |QnA_pairs| that contains exactly 5 elements.\n",
      "\n",
      "Processing:\n",
      "\n",
      "1. Process each element according to the provided instructions.\n",
      "2. Generate and return a JSON array containing exactly 5 results, preserving the original order.\n",
      "3. Your output must strictly adhere to the following JSON schema:\n",
      "[\n",
      "    {\n",
      "    \"accuracy\": number  //  (required),\n",
      "    \"funny\": number  //  (required),\n",
      "    \"conciseness\": number  //  (required),\n",
      "    \"rank\": number  //  (required)\n",
      "    }\n",
      "    ...\n",
      "]\n",
      "\n",
      "Required fields: accuracy, funny, conciseness, rank\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "End of prompt\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(second_llm.render_prompt_printable(QnA_pairs=first_response.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-14 09:26:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.factory\u001b[0m | \u001b[34mfactory.py:180\u001b[0m | \u001b[1m\n",
      "2. Creating master job...\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "Processing city: New York!\n",
      "Processing city: Los Angeles!\n",
      "Processing city: Chicago!\n",
      "Processing city: Houston!\n",
      "Processing city: Miami!\n",
      "\u001b[32m2025-04-14 09:26:13\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:14\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:15\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:20\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:21\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:23\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:25\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.job_manager\u001b[0m | \u001b[34mjob_manager.py:154\u001b[0m | \u001b[1mNo task to run, waiting for task to complete\u001b[0m\n",
      "\u001b[32m2025-04-14 09:26:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.core.data_factory.factory\u001b[0m | \u001b[34mfactory.py:258\u001b[0m | \u001b[1mMaster job ffc15372-77f1-4f1c-bcdf-1b70cce625bb as completed\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">/Users/zhengisamazing/Library/Caches/pypoetry/virtualenvs/starfish-T7IInzTH-py3.11/lib/python3.11/site-packages/ric\n",
       "h/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n",
       "</pre>\n"
      ],
      "text/plain": [
       "/Users/zhengisamazing/Library/Caches/pypoetry/virtualenvs/starfish-T7IInzTH-py3.11/lib/python3.11/site-packages/ric\n",
       "h/live.py:231: UserWarning: install \"ipywidgets\" for Jupyter support\n",
       "  warnings.warn('install \"ipywidgets\" for Jupyter support')\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from starfish.core.llm.utils import merge_structured_outputs \n",
    "@data_factory(max_concurrency=50)\n",
    "async def workflow(city_name, num_records_per_city):\n",
    "    print(f\"Processing city: {city_name}!\")\n",
    "    first_llm = StructuredLLM(\n",
    "        model_name=\"openai/gpt-4o-mini\",\n",
    "        prompt=\"Facts about city {{city_name}}.\",\n",
    "        output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    )\n",
    "\n",
    "    first_response = await first_llm.run(city_name=city_name, num_records=num_records_per_city)\n",
    "\n",
    "\n",
    "    second_llm = StructuredLLM(\n",
    "        model_name=\"openai/gpt-4o-mini\",\n",
    "        prompt=\"\"\"You will be given a question and answer pair, \n",
    "                please rate each individually about accuracy, funny and conciseness. \n",
    "                rating are from 1 to 10, 1 being the worst and 10 being the best. \n",
    "                Here is question and answer pair: {{QnA_pairs}}\"\"\",\n",
    "        output_schema=[{\"name\": \"accuracy\", \"type\": \"int\"}, {\"name\": \"funny\", \"type\": \"int\"}, {\"name\": \"conciseness\", \"type\": \"int\"}],\n",
    "        model_kwargs={\"temperature\": 1},\n",
    "    )\n",
    "\n",
    "    second_response = await second_llm.run(QnA_pairs=first_response.data)\n",
    "\n",
    "    ### Merge result:\n",
    "    final_output = merge_structured_outputs(first_response.data, second_response.data)\n",
    "\n",
    "    return final_output\n",
    "\n",
    "\n",
    "final_output = workflow.run(data = [\n",
    "                                        {'city_name': 'New York'},\n",
    "                                        {'city_name': 'Los Angeles'},\n",
    "                                        {'city_name': 'Chicago'},\n",
    "                                        {'city_name': 'Houston'},\n",
    "                                        {'city_name': 'Miami'}\n",
    "                                    ], num_records_per_city=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "data_factory() got an unexpected keyword argument 'state'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mhandle_duplicate_record\u001b[39m(data: Any, state: Dict[\u001b[38;5;28mstr\u001b[39m, Any]):\n\u001b[32m     18\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRecord duplicated: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m \u001b[38;5;129m@data_factory\u001b[39m\u001b[43m(\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlocal\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_concurrency\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_record_complete\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle_record_complete\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhandle_duplicate_record\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_record_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mhandle_error\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_city_info_wf\u001b[39m(city_name, region_code):\n\u001b[32m     25\u001b[39m     structured_llm = StructuredLLM(\n\u001b[32m     26\u001b[39m         model_name=\u001b[33m\"\u001b[39m\u001b[33mopenai/gpt-4o-mini\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     27\u001b[39m         prompt=\u001b[33m\"\u001b[39m\u001b[33mFacts about city \u001b[39m\u001b[33m{{\u001b[39m\u001b[33mcity_name}} in region \u001b[39m\u001b[33m{{\u001b[39m\u001b[33mregion_code}}.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     28\u001b[39m         output_schema=[{\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m}, {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mstr\u001b[39m\u001b[33m\"\u001b[39m}],\n\u001b[32m     29\u001b[39m         model_kwargs={\u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.7\u001b[39m},\n\u001b[32m     30\u001b[39m     )\n\u001b[32m     31\u001b[39m     output = \u001b[38;5;28;01mawait\u001b[39;00m structured_llm.run(city_name=city_name, region_code=region_code)\n",
      "\u001b[31mTypeError\u001b[39m: data_factory() got an unexpected keyword argument 'state'"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict\n",
    "\n",
    "from starfish import StructuredLLM, data_factory\n",
    "\n",
    "# -> List[Dict] job hooks hook-up\n",
    "\n",
    "\n",
    "# Add callback for error handling\n",
    "def handle_error(err_str: str):\n",
    "    print(f\"Error occurred: {err_str}\")\n",
    "\n",
    "\n",
    "async def handle_record_complete(data: Any, state: Dict[str, Any]):\n",
    "    print(f\"Record complete: {data}\")\n",
    "\n",
    "\n",
    "async def handle_duplicate_record(data: Any, state: Dict[str, Any]):\n",
    "    print(f\"Record duplicated: {data}\")\n",
    "\n",
    "\n",
    "@data_factory(\n",
    "    storage=\"local\", max_concurrency=50, state={}, on_record_complete=[handle_record_complete, handle_duplicate_record], on_record_error=[handle_error]\n",
    ")\n",
    "async def get_city_info_wf(city_name, region_code):\n",
    "    structured_llm = StructuredLLM(\n",
    "        model_name=\"openai/gpt-4o-mini\",\n",
    "        prompt=\"Facts about city {{city_name}} in region {{region_code}}.\",\n",
    "        output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "        model_kwargs={\"temperature\": 0.7},\n",
    "    )\n",
    "    output = await structured_llm.run(city_name=city_name, region_code=region_code)\n",
    "    return output.data\n",
    "\n",
    "\n",
    "# Execute with batch processing\n",
    "results = get_city_info_wf.run(\n",
    "    cities=[\"Paris\", \"Tokyo\", \"New York\", \"London\"],\n",
    "    num_facts=3\n",
    ")\n",
    "\n",
    "results = get_city_info_wf.run(\n",
    "    city_name= [\"Berlin\", \"Rome\"],\n",
    "    region_code=[\"DE\", \"IT\"],\n",
    ")\n",
    "\n",
    "\n",
    "results = get_city_info_wf.run(\n",
    "    data=[{\"city_name\": \"Berlin\"}, {\"city_name\": \"Rome\"}],\n",
    "    region_code=[\"DE\", \"IT\"],\n",
    "    city_name=\"Beijing\",  ### Overwrite the data key\n",
    "    # num_records_per_city = 3\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starfish-T7IInzTH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
