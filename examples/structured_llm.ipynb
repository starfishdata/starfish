{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Version: [Open this notebook in Google Colab](https://colab.research.google.com/github/starfishdata/starfish/blob/main/examples/structured_llm.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix for Jupyter Notebook only ‚Äî do NOT use in production\n",
    "## Enables async code execution in notebooks, but may cause issues with sync/async issues\n",
    "## For production, please run in standard .py files without this workaround\n",
    "## See: https://github.com/erdewit/nest_asyncio for more details\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "from starfish import StructuredLLM\n",
    "from starfish.common.env_loader import load_env_file ## Load environment variables from .env file\n",
    "\n",
    "load_env_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured LLM - Single\n",
    "#### 1. Model provider LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the nickname of New York City?',\n",
       "  'answer': 'The Big Apple'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\")\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù CONSTRUCTED MESSAGES:\n",
      "================================================================================\n",
      "\n",
      "Role: user\n",
      "Content:\n",
      "Facts about city New York.\n",
      "\n",
      "\n",
      "You are asked to generate exactly 5 records and please return the data in the following JSON format:\n",
      "[\n",
      "    {\n",
      "    \"question\": \"\"  //  (required),\n",
      "    \"answer\": \"\"  //  (required)\n",
      "    }\n",
      "    ...\n",
      "]\n",
      "\n",
      "Required fields: question, answer\n",
      "\n",
      "\n",
      "================================================================================\n",
      "End of prompt\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(first_llm.render_prompt_printable(city_name=\"New York\", num_records=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Customized Openai Compatible Model provider LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the nickname of New York City?',\n",
       "  'answer': 'The Big Apple'},\n",
       " {'question': 'Which famous park is located in the center of Manhattan?',\n",
       "  'answer': 'Central Park'},\n",
       " {'question': 'What is the name of the tallest building in New York City?',\n",
       "  'answer': 'One World Trade Center'},\n",
       " {'question': 'Which iconic statue is located in New York Harbor?',\n",
       "  'answer': 'Statue of Liberty'},\n",
       " {'question': 'What is the name of the famous theater district in NYC?',\n",
       "  'answer': 'Broadway'}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_llm = StructuredLLM(\n",
    "    model_name=\"hyperbolic/deepseek-ai/DeepSeek-V3-0324\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 22:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.proxy.litellm_adapter\u001b[0m | \u001b[34mlitellm_adapter.py:94\u001b[0m | \u001b[1mEnsuring Ollama model gemma3:1b is ready...\u001b[0m\n",
      "\u001b[32m2025-04-16 22:34:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:63\u001b[0m | \u001b[1mStarting Ollama server...\u001b[0m\n",
      "\u001b[32m2025-04-16 22:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:79\u001b[0m | \u001b[1mOllama server started successfully\u001b[0m\n",
      "\u001b[32m2025-04-16 22:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:129\u001b[0m | \u001b[1mFound model gemma3:1b\u001b[0m\n",
      "\u001b[32m2025-04-16 22:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:232\u001b[0m | \u001b[1mModel gemma3:1b is already available\u001b[0m\n",
      "\u001b[32m2025-04-16 22:34:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.proxy.litellm_adapter\u001b[0m | \u001b[34mlitellm_adapter.py:103\u001b[0m | \u001b[1mModel gemma3:1b is ready, making API call...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, the population of New York City is approximately 8.8 million people.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Local model\n",
    "first_llm = StructuredLLM(\n",
    "    model_name=\"ollama/gemma3:1b\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 22:34:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:254\u001b[0m | \u001b[1mStopping Ollama server...\u001b[0m\n",
      "\u001b[32m2025-04-16 22:34:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:305\u001b[0m | \u001b[1mOllama server stopped successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Clean it up\n",
    "from starfish.llm.backend.ollama_adapter import stop_ollama_server\n",
    "\n",
    "await stop_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured LLM - Workflow\n",
    "#### 1. Two LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, New York City has an estimated population of over 8.6 million people.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 1},\n",
       " {'question': 'What is the most famous park in New York City?',\n",
       "  'answer': 'Central Park is the most famous park in New York City, spanning 843 acres in the heart of Manhattan.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 2},\n",
       " {'question': 'What is the significance of Times Square?',\n",
       "  'answer': \"Times Square is known as 'The Crossroads of the World' and is famous for its bright lights, Broadway theaters, and New Year's Eve ball drop.\",\n",
       "  'accuracy': 10,\n",
       "  'funny': 3,\n",
       "  'conciseness': 9,\n",
       "  'rank': 3},\n",
       " {'question': 'What iconic statue can be found in New York Harbor?',\n",
       "  'answer': 'The Statue of Liberty, a symbol of freedom and democracy, is located on Liberty Island in New York Harbor.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 4},\n",
       " {'question': 'Which famous bridge connects Manhattan and Brooklyn?',\n",
       "  'answer': 'The Brooklyn Bridge is the iconic bridge that connects Manhattan and Brooklyn over the East River.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 3,\n",
       "  'conciseness': 9,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starfish import StructuredLLM\n",
    "from starfish.llm.utils import merge_structured_outputs\n",
    "\n",
    "first_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "\n",
    "\n",
    "second_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"\"\"You will be given a list of question and answer pairs,\n",
    "please rate each individually about its accuracy, funny and conciseness.\n",
    "rating are from 1 to 10, 1 being the worst and 10 being the best.\n",
    "lets also rank them among themself so from 1 being the best.\n",
    "Here is question and answer pairs: {{QnA_pairs}}\"\"\",\n",
    "    output_schema=[\n",
    "        {\"name\": \"accuracy\", \"type\": \"int\"},\n",
    "        {\"name\": \"funny\", \"type\": \"int\"},\n",
    "        {\"name\": \"conciseness\", \"type\": \"int\"},\n",
    "        {\"name\": \"rank\", \"type\": \"int\"},\n",
    "    ],\n",
    "    model_kwargs={\"temperature\": 1},\n",
    ")\n",
    "\n",
    "second_response = await second_llm.run(QnA_pairs=first_response.data)\n",
    "\n",
    "### Merge result:\n",
    "merge_structured_outputs(first_response.data, second_response.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starfish-T7IInzTH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
