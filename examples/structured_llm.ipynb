{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix for Jupyter Notebook only ‚Äî do NOT use in production\n",
    "## Allows nested async loops (e.g., for asyncio in notebooks) ‚Äî not needed in normal .py files\n",
    "## For production please use normal .py files!!! \n",
    "## See: https://github.com/erdewit/nest_asyncio for more details\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 22:24:34\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.common.env_loader\u001b[0m | \u001b[34menv_loader.py:52\u001b[0m | \u001b[1mLoaded 9 environment variables from /Users/zhengisamazing/1.python_dir/starfish/.env\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starfish import StructuredLLM, data_factory\n",
    "from starfish.common.env_loader import load_env_file\n",
    "\n",
    "load_env_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured LLM - Single\n",
    "#### 1. Model provider LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, New York City has an estimated population of over 8.4 million people, making it the most populous city in the United States.'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\")\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "üìù CONSTRUCTED MESSAGES:\n",
      "================================================================================\n",
      "\n",
      "Role: user\n",
      "Content:\n",
      "Facts about city New York.\n",
      "\n",
      "\n",
      "You are asked to generate exactly 5 records and please return the data in the following JSON format:\n",
      "[\n",
      "    {\n",
      "    \"question\": \"\"  //  (required),\n",
      "    \"answer\": \"\"  //  (required)\n",
      "    }\n",
      "    ...\n",
      "]\n",
      "\n",
      "Required fields: question, answer\n",
      "\n",
      "\n",
      "================================================================================\n",
      "End of prompt\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(first_llm.render_prompt_printable(city_name=\"New York\", num_records=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Customized Openai Compatible Model provider LLM Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the nickname of New York City?',\n",
       "  'answer': 'The Big Apple'},\n",
       " {'question': 'Which famous park is located in the center of Manhattan?',\n",
       "  'answer': 'Central Park'},\n",
       " {'question': 'What is the name of the tallest building in New York City?',\n",
       "  'answer': 'One World Trade Center'},\n",
       " {'question': 'Which iconic statue is located in New York Harbor?',\n",
       "  'answer': 'Statue of Liberty'},\n",
       " {'question': 'What is the name of the famous theater district in Manhattan?',\n",
       "  'answer': 'Broadway'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_llm = StructuredLLM(\n",
    "    model_name=\"hyperbolic/deepseek-ai/DeepSeek-V3-0324\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 22:24:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.proxy.litellm_adapter\u001b[0m | \u001b[34mlitellm_adapter.py:94\u001b[0m | \u001b[1mEnsuring Ollama model gemma3:1b is ready...\u001b[0m\n",
      "\u001b[32m2025-04-16 22:24:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:63\u001b[0m | \u001b[1mStarting Ollama server...\u001b[0m\n",
      "\u001b[32m2025-04-16 22:24:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:79\u001b[0m | \u001b[1mOllama server started successfully\u001b[0m\n",
      "\u001b[32m2025-04-16 22:24:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:129\u001b[0m | \u001b[1mFound model gemma3:1b\u001b[0m\n",
      "\u001b[32m2025-04-16 22:24:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:232\u001b[0m | \u001b[1mModel gemma3:1b is already available\u001b[0m\n",
      "\u001b[32m2025-04-16 22:24:46\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.proxy.litellm_adapter\u001b[0m | \u001b[34mlitellm_adapter.py:103\u001b[0m | \u001b[1mModel gemma3:1b is ready, making API call...\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City?',\n",
       "  'answer': 'As of 2023, the population of New York City is approximately 8.8 million people.'}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Local model\n",
    "first_llm = StructuredLLM(\n",
    "    model_name=\"ollama/gemma3:1b\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    "    model_kwargs={\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "first_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-16 22:24:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:254\u001b[0m | \u001b[1mStopping Ollama server...\u001b[0m\n",
      "\u001b[32m2025-04-16 22:24:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mstarfish.llm.backend.ollama_adapter\u001b[0m | \u001b[34mollama_adapter.py:305\u001b[0m | \u001b[1mOllama server stopped successfully\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Clean it up\n",
    "from starfish.llm.backend.ollama_adapter import stop_ollama_server\n",
    "\n",
    "await stop_ollama_server()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Structured LLM - Workflow\n",
    "#### 1. Two LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What is the population of New York City as of 2023?',\n",
       "  'answer': 'As of 2023, the estimated population of New York City is approximately 8.5 million people.',\n",
       "  'accuracy': 9,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 1},\n",
       " {'question': 'What is the tallest building in New York City?',\n",
       "  'answer': 'The tallest building in New York City is One World Trade Center, which stands at 1,776 feet tall.',\n",
       "  'accuracy': 9,\n",
       "  'funny': 2,\n",
       "  'conciseness': 9,\n",
       "  'rank': 2},\n",
       " {'question': 'What famous park is located in the heart of Manhattan?',\n",
       "  'answer': 'Central Park is the famous park located in the heart of Manhattan.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 10,\n",
       "  'rank': 3},\n",
       " {'question': 'Which bridge connects Manhattan and Brooklyn?',\n",
       "  'answer': 'The Brooklyn Bridge connects Manhattan and Brooklyn.',\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 10,\n",
       "  'rank': 4},\n",
       " {'question': 'What is the nickname of New York City?',\n",
       "  'answer': \"New York City is commonly known as 'The Big Apple.'\",\n",
       "  'accuracy': 10,\n",
       "  'funny': 2,\n",
       "  'conciseness': 10,\n",
       "  'rank': 5}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from starfish import StructuredLLM\n",
    "from starfish.llm.utils import merge_structured_outputs\n",
    "\n",
    "first_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"Facts about city {{city_name}}.\",\n",
    "    output_schema=[{\"name\": \"question\", \"type\": \"str\"}, {\"name\": \"answer\", \"type\": \"str\"}],\n",
    ")\n",
    "\n",
    "first_response = await first_llm.run(city_name=\"New York\", num_records=5)\n",
    "\n",
    "\n",
    "second_llm = StructuredLLM(\n",
    "    model_name=\"openai/gpt-4o-mini\",\n",
    "    prompt=\"\"\"You will be given a list of question and answer pairs,\n",
    "please rate each individually about its accuracy, funny and conciseness.\n",
    "rating are from 1 to 10, 1 being the worst and 10 being the best.\n",
    "lets also rank them among themself so from 1 being the best.\n",
    "Here is question and answer pairs: {{QnA_pairs}}\"\"\",\n",
    "    output_schema=[\n",
    "        {\"name\": \"accuracy\", \"type\": \"int\"},\n",
    "        {\"name\": \"funny\", \"type\": \"int\"},\n",
    "        {\"name\": \"conciseness\", \"type\": \"int\"},\n",
    "        {\"name\": \"rank\", \"type\": \"int\"},\n",
    "    ],\n",
    "    model_kwargs={\"temperature\": 1},\n",
    ")\n",
    "\n",
    "second_response = await second_llm.run(QnA_pairs=first_response.data)\n",
    "\n",
    "### Merge result:\n",
    "merge_structured_outputs(first_response.data, second_response.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starfish-T7IInzTH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
