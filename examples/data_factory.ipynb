{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Colab Version: [Open this notebook in Google Colab](https://colab.research.google.com/github/starfishdata/starfish/blob/main/examples/data_factory.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install starfish-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fix for Jupyter Notebook only — do NOT use in production\n",
    "## Enables async code execution in notebooks, but may cause issues with sync/async issues\n",
    "## For production, please run in standard .py files without this workaround\n",
    "## See: https://github.com/erdewit/nest_asyncio for more details\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from starfish import StructuredLLM, data_factory\n",
    "from starfish.llm.utils import merge_structured_outputs\n",
    "\n",
    "from starfish.common.env_loader import load_env_file ## Load environment variables from .env file\n",
    "load_env_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup your openai api key if not already set\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"your_key_here\"\n",
    "\n",
    "# If you dont have any API key, please navigate to local model section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function mock llm call\n",
    "# When developing data pipelines with LLMs, making thousands of real API calls\n",
    "# can be expensive. Using mock LLM calls lets you test your pipeline's reliability,\n",
    "# failure handling, and recovery without spending money on API calls.\n",
    "from starfish.data_factory.utils.mock import mock_llm_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Your First Data Factory: Simple Scaling\n",
    "\n",
    "The @data_factory decorator transforms any async function into a scalable data processing pipeline.\n",
    "It handles:\n",
    "- Parallel execution \n",
    "- Automatic batching\n",
    "- Error handling & retries\n",
    "- Progress tracking\n",
    "\n",
    "Let's start with a single LLM call and then show how easy it is to scale it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'fact': \"In New York City, it's illegal to honk your horn unless it's an emergency—so if you hear a lot of honking, someone must be in a real rush!\"}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, create a StructuredLLM instance for generating facts about cities\n",
    "json_llm = StructuredLLM(\n",
    "    model_name = \"openai/gpt-4o-mini\",\n",
    "    prompt = \"Funny facts about city {{city_name}}.\",\n",
    "    output_schema = [{'name': 'fact', 'type': 'str'}],\n",
    "    model_kwargs = {\"temperature\": 0.7},\n",
    ")\n",
    "\n",
    "json_llm_response = await json_llm.run(city_name='New York')\n",
    "json_llm_response.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 09:50:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: b417d8f3-c8cb-4a21-b6c4-15f862a7db31\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 09:50:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/5\u001b[0m | \u001b[33mRunning: 5\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "Processing New York at 2025-04-17 09:50:06.012056\n",
      "Processing London at 2025-04-17 09:50:06.012331\n",
      "Processing Tokyo at 2025-04-17 09:50:06.013621\n",
      "Processing Paris at 2025-04-17 09:50:06.013810\n",
      "Processing Sydney at 2025-04-17 09:50:06.013972\n",
      "\u001b[32m2025-04-17 09:50:09\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 5/5\u001b[0m | \u001b[33mAttempted: 5\u001b[0m (Failed: 0, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'fact': \"Sydney's famous Bondi Beach is so popular that it even has its own social media influencer - a dog named 'Hugo' who has over 100,000 followers on Instagram!\"},\n",
       " {'fact': 'In Paris, there are more dogs than children! With around 300,000 pups frolicking around the city, it seems like the dogs might just be winning the battle for the best city pet.'},\n",
       " {'fact': \"London has a 'Pigeon Pie' shop where you can buy pies made with actual pigeon meat, which might explain why the city's pigeons are so well-fed and sassy!\"},\n",
       " {'fact': \"Tokyo has a 'Cat Island' called Aoshima, where more cats outnumber humans 6 to 1, making it a purr-fect getaway for feline lovers!\"},\n",
       " {'fact': \"New Yorkers are so busy that they often walk faster than the speed limit, which is usually 25 mph—making them the only pedestrians in the world who might get a ticket for being 'too fast'!\"}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, scale to multiple cities using data_factory\n",
    "# Just add the @data_factory decorator to process many cities in parallel\n",
    "\n",
    "from datetime import datetime\n",
    "@data_factory(max_concurrency=10)\n",
    "async def process_json_llm(city_name: str):\n",
    "    ## Adding a print statement to indicate the start of the processing\n",
    "    print(f\"Processing {city_name} at {datetime.now()}\")\n",
    "    json_llm_response = await json_llm.run(city_name=city_name)\n",
    "    return json_llm_response.data\n",
    "\n",
    "# This is all it takes to scale from one city to many cities!\n",
    "process_json_llm(city_name=[\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Works with any aysnc function\n",
    "\n",
    "Data Factory works with any async function, not just LLM calls, you can build complex pipelines involving multiple LLMs, data processing, etc.\n",
    "\n",
    "Here is example of two chained structured llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 10:08:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: 5061522d-ff62-46ce-87fe-39c59cc6227c\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 10:08:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/3\u001b[0m | \u001b[33mRunning: 3\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:08:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/3\u001b[0m | \u001b[33mRunning: 3\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:08:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 2/3\u001b[0m | \u001b[33mRunning: 1\u001b[0m | \u001b[36mAttempted: 2\u001b[0m    (\u001b[32mCompleted: 2\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:08:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 3/3\u001b[0m | \u001b[33mAttempted: 3\u001b[0m (Failed: 0, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Example of a more complex function that chains multiple LLM calls\n",
    "# This was grabbed from structured llm examples \n",
    "\n",
    "@data_factory(max_concurrency=10)\n",
    "async def complex_process_cities(topic: str):\n",
    "    ## topic → generator_llm → rating_llm → merged results\n",
    "    # First LLM to generate question/answer pairs\n",
    "    generator_llm = StructuredLLM(\n",
    "        model_name=\"openai/gpt-4o-mini\",\n",
    "        prompt=\"Generate question/answer pairs about {{topic}}.\",\n",
    "        output_schema=[\n",
    "            {\"name\": \"question\", \"type\": \"str\"},\n",
    "            {\"name\": \"answer\", \"type\": \"str\"}\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    # Second LLM to rate the generated pairs\n",
    "    rater_llm = StructuredLLM(\n",
    "        model_name=\"openai/gpt-4o-mini\",\n",
    "        prompt='''Rate the following Q&A pairs based on accuracy and clarity (1-10).\n",
    "        Pairs: {{generated_pairs}}''',\n",
    "        output_schema=[\n",
    "            {\"name\": \"accuracy_rating\", \"type\": \"int\"},\n",
    "            {\"name\": \"clarity_rating\", \"type\": \"int\"}\n",
    "        ],\n",
    "        model_kwargs={\"temperature\": 0.5}\n",
    ")\n",
    "\n",
    "    generation_response = await generator_llm.run(topic=topic, num_records=5)\n",
    "    rating_response = await rater_llm.run(generated_pairs=generation_response.data)\n",
    "    \n",
    "    # Merge the results\n",
    "    return merge_structured_outputs(generation_response.data, rating_response.data)\n",
    "\n",
    "\n",
    "### To save on token here we only use 3 topics as example\n",
    "complex_process_cities_data = complex_process_cities.run(topic=['Science', 'History', 'Technology'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "[{'question': 'What is the chemical symbol for water?', 'answer': 'H2O', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What planet is known as the Red Planet?', 'answer': 'Mars', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the process by which plants make their food using sunlight?', 'answer': 'Photosynthesis', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the center of an atom called?', 'answer': 'Nucleus', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What gas do living beings need to breathe?', 'answer': 'Oxygen', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the largest planet in our solar system?', 'answer': 'Jupiter is the largest planet in our solar system.', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the process by which plants make their food?', 'answer': 'Plants make their food through a process called photosynthesis.', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the chemical symbol for gold?', 'answer': 'The chemical symbol for gold is Au.', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What gas do living creatures need to breathe?', 'answer': 'Living creatures need oxygen to breathe.', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': \"What is Newton's second law of motion?\", 'answer': \"Newton's second law of motion states that the force acting on an object is equal to the mass of that object multiplied by its acceleration (F = ma).\", 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the process by which plants convert sunlight into energy?', 'answer': 'Photosynthesis is the process by which plants convert sunlight into energy, using carbon dioxide and water to produce glucose and oxygen.', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the smallest unit of life that can function independently?', 'answer': 'The smallest unit of life that can function independently is the cell.', 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': \"What is Newton's second law of motion?\", 'answer': \"Newton's second law of motion states that the acceleration of an object is directly proportional to the net force acting on it and inversely proportional to its mass, often summarized by the equation F = ma.\", 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': \"What is the primary gas found in the Earth's atmosphere?\", 'answer': \"The primary gas found in the Earth's atmosphere is nitrogen, which makes up about 78% of the atmosphere.\", 'accuracy_rating': 10, 'clarity_rating': 10}, {'question': 'What is the theory of evolution by natural selection?', 'answer': 'The theory of evolution by natural selection, proposed by Charles Darwin, suggests that organisms better adapted to their environment are more likely to survive and reproduce, leading to the gradual evolution of species over time.', 'accuracy_rating': 10, 'clarity_rating': 10}]\n"
     ]
    }
   ],
   "source": [
    "### Each topic has 5 question/answer pairs so 3 topics has 15 pairs!\n",
    "print(len(complex_process_cities_data))\n",
    "print(complex_process_cities_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Working with Different Input Formats\n",
    "\n",
    "\n",
    "Data Factory is flexible with how you provide inputs. Let's demonstrate different ways to pass parameters to data_factory functions.\n",
    "\n",
    "'data' is a reserved keyword expecting list(dict) or tuple(dict) - this design make it super easy to pass large data and support HuggingFace and Pandas dataframe very easily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'answer': 'New York_5'}, {'answer': 'New York_4'}, {'answer': 'New York_5'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## We will be using mock llm call for rest of example to save on token\n",
    "## Mock LLM call is a function that simulates an LLM API call with random delays (controlled by sleep_time) and occasional failures (controlled by fail_rate)\n",
    "await mock_llm_call(city_name=\"New York\", num_records_per_city=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "@data_factory(max_concurrency=100)\n",
    "async def input_format_mock_llm(city_name: str, num_records_per_city: int):\n",
    "    return await mock_llm_call(city_name=city_name, num_records_per_city=num_records_per_city, fail_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format 1: Multiple lists that get zipped together\n",
    "input_format_data1 = input_format_mock_llm.run(city_name=[\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\"], num_records_per_city=[2, 1, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 09:50:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: 7d742262-e631-4e8f-9da1-fe755cfdd112\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 09:50:17\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/5\u001b[0m | \u001b[33mRunning: 5\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 09:50:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 5/5\u001b[0m | \u001b[33mAttempted: 5\u001b[0m (Failed: 0, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Format 2: List + single value (single value gets broadcasted)\n",
    "input_format_data2 = input_format_mock_llm.run(city_name=[\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\"], num_records_per_city=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 09:50:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: db640456-827f-4df2-8463-b4f1fd9d6f9c\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 09:50:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/5\u001b[0m | \u001b[33mRunning: 5\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 09:50:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 5/5\u001b[0m | \u001b[33mAttempted: 5\u001b[0m (Failed: 0, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Format 3: Special 'data' parameter\n",
    "# 'data' is a reserved keyword expecting list(dict) or tuple(dict)\n",
    "# Makes integration with various data sources easier\n",
    "input_format_data3 = input_format_mock_llm.run(data=[{\"city_name\": \"New York\", \"num_records_per_city\": 2}, {\"city_name\": \"London\", \"num_records_per_city\": 1}, {\"city_name\": \"Tokyo\", \"num_records_per_city\": 1}, {\"city_name\": \"Paris\", \"num_records_per_city\": 1}, {\"city_name\": \"Sydney\", \"num_records_per_city\": 1}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Resilient error retry\n",
    "Data Factory automatically handles errors and retries, making your pipelines robust.\n",
    "\n",
    "Let's demonstrate with a high failure rate example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 10:12:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: 3ac13811-28b9-4a95-b819-102d9a616040\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/25\u001b[0m | \u001b[33mRunning: 25\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 20/25\u001b[0m | \u001b[33mRunning: 5\u001b[0m | \u001b[36mAttempted: 20\u001b[0m    (\u001b[32mCompleted: 20\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:07\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError running task: Mock LLM failed to process city: Tokyo\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:07\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError running task: Mock LLM failed to process city: Sydney\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:07\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError running task: Mock LLM failed to process city: New York\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:07\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError running task: Mock LLM failed to process city: London\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:10\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 24/25\u001b[0m | \u001b[33mRunning: 1\u001b[0m | \u001b[36mAttempted: 28\u001b[0m    (\u001b[32mCompleted: 24\u001b[0m, \u001b[31mFailed: 4\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:12:12\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 25/25\u001b[0m | \u001b[33mAttempted: 29\u001b[0m (Failed: 4, Filtered: 0, Duplicate: 0)\u001b[0m\n",
      "\n",
      "Successfully completed 25 out of 25 tasks\n",
      "Data Factory automatically handled the failures and continued processing\n",
      "The results only include successful tasks\n"
     ]
    }
   ],
   "source": [
    "@data_factory(max_concurrency=100)\n",
    "async def high_error_rate_mock_llm(city_name: str, num_records_per_city: int):\n",
    "    return await mock_llm_call(city_name=city_name, num_records_per_city=num_records_per_city, fail_rate=0.3) # Hardcode to 30% chance of failure\n",
    "\n",
    "# Process all cities - some will fail, but data_factory keeps going\n",
    "cities = [\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\"] * 5  # 25 cities\n",
    "high_error_rate_mock_lllm_data = high_error_rate_mock_llm.run(city_name=cities, num_records_per_city=1)\n",
    "\n",
    "print(f\"\\nSuccessfully completed {len(high_error_rate_mock_lllm_data)} out of {len(cities)} tasks\")\n",
    "print(\"Data Factory automatically handled the failures and continued processing\")\n",
    "print(\"The results only include successful tasks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Re run\n",
    "\n",
    "If a job is interrupted, you can pick up where you left off using re_run().\n",
    "\n",
    "This is essential for long-running jobs with thousands of tasks.\n",
    "\n",
    "We're simulating an interruption here. In a real scenario, this might happen if your notebook kernel crashes or you stop execution manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 10:13:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: dc2a08a0-3267-482d-a817-3d1a5c637ef1\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:41\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/100\u001b[0m | \u001b[33mRunning: 10\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 21/100\u001b[0m | \u001b[33mRunning: 10\u001b[0m | \u001b[36mAttempted: 21\u001b[0m    (\u001b[32mCompleted: 21\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:44\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError running task: Mock LLM failed to process city: New York\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError running task: Mock LLM failed to process city: New York\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:45\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError occurred: KeyboardInterrupt\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚨 Job stopped unexpectedly. You can resume the job using master_job_id by re_run(\"dc2a08a0-3267-482d-a817-3d1a5c637ef1\")\u001b[0m\n",
      "\u001b[32m2025-04-17 10:13:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 27/100\u001b[0m | \u001b[33mAttempted: 29\u001b[0m (Failed: 2, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "@data_factory(max_concurrency=10)\n",
    "async def re_run_mock_llm(city_name: str, num_records_per_city: int):\n",
    "    return await mock_llm_call(city_name=city_name, num_records_per_city=num_records_per_city, fail_rate=0.3)\n",
    "\n",
    "cities = [\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\"] * 20  # 100 cities\n",
    "re_run_mock_llm_data_1 = re_run_mock_llm.run(city_name=cities, num_records_per_city=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When a job is interrupted, you'll see a message like:\n",
      "'🚨 Job stopped unexpectedly. You can resume the job using master_job_id'\n",
      "'by re_run(\"9a590f0f-994d-4d3b-b2db-30e75143ff14\")'\n",
      "\n",
      "To resume an interrupted job, simply call:\n",
      "interrupted_job_mock_llm.re_run(\"YOUR_JOB_ID\")\n",
      "\n",
      "For this example we have 27/100 data generated and not finished yet!\n"
     ]
    }
   ],
   "source": [
    "print(\"When a job is interrupted, you'll see a message like:\")\n",
    "print(\"'🚨 Job stopped unexpectedly. You can resume the job using master_job_id'\")\n",
    "print(\"'by re_run(\\\"9a590f0f-994d-4d3b-b2db-30e75143ff14\\\")'\")\n",
    "\n",
    "print(\"\\nTo resume an interrupted job, simply call:\")\n",
    "print(\"interrupted_job_mock_llm.re_run(\\\"YOUR_JOB_ID\\\")\")\n",
    "print('')\n",
    "print(f\"For this example we have {len(re_run_mock_llm_data_1)}/{len(cities)} data generated and not finished yet!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 10:15:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[33mPICKING UP FROM WHERE THE JOB WAS LEFT OFF...\u001b[0m\n",
      "\u001b[1mSTATUS AT THE TIME OF RE-RUN:\u001b[0m \u001b[32mCompleted: 95 / 100\u001b[0m | \u001b[31mFailed: 12\u001b[0m | \u001b[31mDuplicate: 0\u001b[0m | \u001b[33mFiltered: 0\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 10:15:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 95/100\u001b[0m | \u001b[33mRunning: 5\u001b[0m | \u001b[36mAttempted: 107\u001b[0m    (\u001b[32mCompleted: 95\u001b[0m, \u001b[31mFailed: 12\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:15:04\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[31m\u001b[1mError occurred: KeyboardInterrupt\u001b[0m\n",
      "\u001b[32m2025-04-17 10:15:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m🚨 Job stopped unexpectedly. You can resume the job using master_job_id by re_run(\"dc2a08a0-3267-482d-a817-3d1a5c637ef1\")\u001b[0m\n",
      "\u001b[32m2025-04-17 10:15:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 100/100\u001b[0m | \u001b[33mAttempted: 112\u001b[0m (Failed: 12, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "## Lets keep continue the rest of run by re_run \n",
    "re_run_mock_llm_data_2 = re_run_mock_llm.re_run(\"dc2a08a0-3267-482d-a817-3d1a5c637ef1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we still able to finished with what is left!! 100 data generated!\n"
     ]
    }
   ],
   "source": [
    "print(f\"Now we still able to finished with what is left!! {len(re_run_mock_llm_data_2)} data generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Dry run\n",
    "Before running a large job, you can do a \"dry run\" to test your pipeline. This only processes a single item and doesn't save state to the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-04-17 10:01:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m\u001b[1mJob START:\u001b[0m \u001b[36mMaster Job ID: None\u001b[0m | \u001b[33mLogging progress every 3 seconds\u001b[0m\u001b[0m\n",
      "\u001b[32m2025-04-17 10:01:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[32mCompleted: 0/1\u001b[0m | \u001b[33mRunning: 1\u001b[0m | \u001b[36mAttempted: 0\u001b[0m    (\u001b[32mCompleted: 0\u001b[0m, \u001b[31mFailed: 0\u001b[0m, \u001b[35mFiltered: 0\u001b[0m, \u001b[34mDuplicate: 0\u001b[0m)\u001b[0m\n",
      "\u001b[32m2025-04-17 10:01:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[1m[JOB PROGRESS] \u001b[1mJob Finished:\u001b[0m \u001b[32mCompleted: 1/0\u001b[0m | \u001b[33mAttempted: 1\u001b[0m (Failed: 0, Filtered: 0, Duplicate: 0)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "@data_factory(max_concurrency=10)\n",
    "async def dry_run_mock_llm(city_name: str, num_records_per_city: int):\n",
    "    return await mock_llm_call(city_name=city_name, num_records_per_city=num_records_per_city, fail_rate=0.3)\n",
    "\n",
    "dry_run_mock_llm_data = dry_run_mock_llm.dry_run(city_name=[\"New York\", \"London\", \"Tokyo\", \"Paris\", \"Sydney\"]*20, num_records_per_city=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Advanced Usage\n",
    "Data Factory offers more advanced capabilities for complete pipeline customization, including hooks that execute at key stages and shareable state to coordinate between tasks. These powerful features enable complex workflows and fine-grained control. Our dedicated examples for advanced data_factory usage will be coming soon!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "starfish-T7IInzTH-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
